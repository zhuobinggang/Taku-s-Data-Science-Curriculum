\documentclass[a4paper]{article}
\usepackage{xeCJK}
\setCJKmainfont{IPAMincho}
\setCJKsansfont{IPAGothic}
\setCJKmonofont{IPAGothic}
\usepackage{float}
\restylefloat{table}
\restylefloat{figure}
\usepackage{graphicx}
\title{企画書}
\date{2020年10月20日}
\author{卓 秉綱}

\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}

\addbibresource{qhe.bib}


\begin{document}
\maketitle
研究テーマ：セクション分割による文書のトピック分割

\section{概要}

一つの文章には通常、複数のトピックが含まれている。例えば、とあるフランスの紹介文章に、第一部分のトピックはフランスの地理環境、第二部分はフランスの歴史である。通常、筆者は段落を分けるが、トピックを意図的に分けることはない。既存のさまざまなテキスト分析システム（例えば、自動要約、情報検索 質問応答）には、この問題が常に無視されている。\\

本研究では、テキスト分析する前にトピックのセグメンテーションを実行すると、より良い成果を出されると想定する。たとえば、トピックごとに自動要約を行い、その結果を組み合わせると、自動要約の精度の向上が期待できる。そのため、新たなトピック分割の手法を提案する。

\section{先行研究}

Arnoldら\cite{arnold2019sector}は、まず文をword2vecで特徴ベクトルに転換し、BiLSTMを使って両方向の文脈に基づきその文を複数のトピックカテゴリに分類する。\\

この研究との違いは：1）word2vecの代わりにSentence-bertを使用して、文の特徴ベクトルを取得する。 2）トピックの分割にはBiLSTMの代わりにポインターネットワークを使用する。評価するとき、両者の結果を比較する。 3）トピックの分類を考慮しない。本研究の目的は、一般的なセクション分割モデルを構築することである。トピック分類には、固定数のカテゴリーラベルが必要で、幅広いアプリケーションシナリオに対応できないため、本研究はトピックを分類しない。


\section{有用性}

我々が提案する訓練手法を通じて、トピック分割モデルを獲得できる。抽出的要約に限らず、トピックを理解したモデルは、自動要約、情報検索質問応答など様々なテキスト分析システムに役立てると想定する。

\section{提案手法}

まず、文全体をSentence-Bert\cite{reimers2019sentence}で特徴ベクトルに転換し、ポインタネットワーク\cite{vinyals2015pointer}に入力してから、分割すべきの文の位置を出力する。その結果に基づき、文書のトピック分割は簡単にできる。\\

トピック分割モデルを予備訓練するために、自己教師あり(self-supervised)の学習手法を提案する：コーパスから文書を読み取り、その文書の全ての段落を一つに合わせ、モデルに段落の再分割をさせる。その訓練により、モデルがトピックへの理解を学習できると期待する。\\

予備訓練した後、WikiSectionを用いてセクション分割をさせてモデルを微調整する。\\

\subsection{Sentence-bert}

Sentence-bertを使って、簡単に文を埋め込みに変換し、各文間のユークリッド距離を比較できる。

\subsubsection{BERTとの比較}
BERTを使って文を埋め込みに変換する場合は、いくつかの裏技を使用しなければならない。先行研究によると、このような方法で得る結果はword2vecの場合とくらべば非常に良いとは言えない。Sentence bertはこの問題を解決するために生まれ、複数のアプリケーションシナリオで最先端（2019）の成果を取得しました。

\subsubsection{word2vecとの比較}
word2vecの訓練は単語単位に基づいており、BERTは文単位（クローズ、上下の文のマッチングなど）で学習されるため、文の埋め込みを取得したい場合、BERTのほうが適切だと思われる。

\subsection{Pointer-network}
ポインタネットワークを使って、図\ref{fig:img2}のように、入力シーケンスへの複数のポインタを出力できる。 本研究では、文章内のすべての文を入力し、ポインタをセクションの分割点として出力する。

\begin{figure}[H]
  \includegraphics[width=\columnwidth]{imgs/ptnw.png}
  \caption{Attentionメカニズムに基づくPointer-network}
  \label{fig:img2}
\end{figure}

\subsection{WikiSection}
このデータセットは、ヴィキペディアに基づいて生成された。 一つのヴィキ記事には複数のサブセクションが存在する。たとえば、フランスに関する記事は、歴史や地理などの複数のサブセクションに分割されている。このデータセットは、それらの記事を使いやすい形に整理した。

\section{実験手法}

\subsection{予備訓練}

まず、インターネットから多数の文章を取得し、文章の全ての段落を一つに合わせ、トピック分割モデルに段落の再分割をさせ、予備訓練する。具体的な流れは図\ref{fig:img1}である。

\begin{figure}[H]
  \includegraphics[width=\columnwidth]{imgs/01.jpg}
  \caption{トピック分割モデルの構造と訓練の流れ}
  \label{fig:img1}
\end{figure}

\subsection{微調整}

予備訓練した後、WikiSectionを使用してトピック分割モデルを微調整する。微調整の手法は予備訓練の時とほぼ同じである。

\subsection{評価方法}

各文が分割位置であるかどうかを判断し、正解率を取得、Fスコアを計算できる。\\

対照として、3つの実験手法を提案する：1）全ての文は分割位置ではないと判断する方法をベースラインとする。2）BERTによるセクション分割。3）biLSTMによるセクション分割。

\printbibliography

\end{document}
