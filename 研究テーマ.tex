\documentclass[a4paper]{article}
\usepackage{xeCJK}
\setCJKmainfont{IPAMincho}
\setCJKsansfont{IPAGothic}
\setCJKmonofont{IPAGothic}
\usepackage{graphicx}

\title{段落分割による文書のトピック分割を用いて重要文抽出}
\date{2020-10}
\author{卓 秉綱}

\begin{document}
\maketitle

\section{研究の背景}

自動要約の目的は、主旨を保つながら文書を短く圧縮すること。自動要約の手法は、抽出的(extractive)要約と生成的(abstractive)要約に分けられる。本研究は抽出的要約に注目する。\\

従来の抽出手法は、一文ずつ読み取り、その文を抽出すべきか否かを2分類する。その手法は文脈を無視し、独立した文だけを考慮する故に良い結果を出せないと思われる。\\

最近、MingZhongらは、新たな抽出手法を提案した。MingZhongらは抽出の任務を「意味的テキストマッチング(semantic text matching)」と再定義し、文と文書全体をBertで特徴ベクトルに転換し、その類似度(ユークリッド距離)に基づき、文を抽出するか否かを決める(画像\ref{fig:img1})。その研究は最先端の成果を出した。\\

しかし、MingZhongらの研究には一つの問題点がある。それは、一つの文書に複数のトピックが存在することであり(例えば、とあるフランスの紹介文章に、第一部分のトピックはフランスの地理環境、第二部分はフランスの歴史)、それに対して、通常、一文は複数のトピックに所属することができない。それゆえ、その文と文書のマッチングはアンバランスということは明らかである。本研究は、その問題は要約に悪影響をもたらすことを仮定する。\\

\begin{figure}
  \includegraphics[]{imgs/01.png}
  \caption{仮想のゴールド要約は、文書とのユークリッド距離は一番近い}
  \label{fig:img1}
\end{figure}

\section{関連研究}

本研究で使ったデータセットWikiSectionは、Arnoldらの最近の研究（《SECTOR: A Neural Model for Coherent Topic Segmentation and Classification》）によるものである。\\

彼らの研究手法は、まず文をword2vecで特徴ベクトルに転換し、BLSTMを使って両方向の文脈に基づきその文を複数のトピックカテゴリに分類する。\\

本研究は、トピックカテゴリを使わず、文を分類せず、分割の位置だけに注目する。\\

しかし、BLSTMを使ってトピックの埋め込み(embedding)を取得できる、そのため、モデルはトピックへの理解が深めるではないかと懸念する。\\

\section{研究目的}

本研究は、トピック分割問題の解決を目指す。\\

\section{提案手法}

上記の問題により、もし先立って文書をトピックに基づき複数の文書に分割し、その複数の文書にMingZhongらの提案手法を用いってから結果を合わせれば、要約精度の向上に役たつと、本研究はそう仮定する。\\

それゆえ、トピック分割モデルを提案する。このモデルは、文を順番に読み取ってからBertで特徴ベクトルに転換し、その文は新たなのトピックの始まりであるかをモデルで二分類させ。その結果に基づき、文書のトピック分割は簡単にできる。\\

トピック分割モデルを予備訓練するために、自己教師あり(self-supervised)の学習手法を提出する：コーパスから文書を読み取り、その文書の全ての段落を一つに合わせ、モデルに段落の再分割をさせる。その訓練により、モデルがトピックへの理解を学習できると期待する。\\

\subsection{可能な問題と解決法}
\begin{itemize}
\item 問題点１：　独立した文だけで段落分割は難しい、文脈関係は必要。
\item 解決法：　その文と前後のN文をモデルに与える。
\item 問題点２：　毎回の決定(decision)は独立であり、無駄、過剰な段落分けが発生する可能性がある。
\item 解決法：　ポインタネットワークを導入。決定同士に影響を与えられる。
\item 問題点３：　微調整のデータセットとやり方。
\item 解決法：　4.4節で説明する。

\end{itemize}

\subsection{ポインタネットワークの導入}

ポインタネットワークは、エンコーダとデコーダで構成され、可変長のn個の入力を受け入れ、複数のインデックス（0 <= index < n）を出力します。\\

本研究の提案手法は、文書のすべての文をポインターネットワークの入力として特徴ベクトルに変換し、ポインターネットワークにすべての段落の最初の文のインデックスを出力させる。\\

エンコーダーはすべての文、つまり文書全体を処理したゆえ、文脈を考慮できる。そして、独立した決定の無駄出力を減らすことができる。\\

\subsection{Bertで文を特徴ベクトルに転換}

Sentence-BERTを使えば、文を特徴ベクトルに転換できる。

\subsection{微調整のデータセットとやり方}

データセットはWikiSectionを使用し、githubで直接取得できる。ウィキペディアの各エントリは複数のサブセクションに分割され、それぞれにサブセクション名が付いている（たとえば、とある病気のエントリは症状と治療方法のサブセクションを含む）。WikiSectionはそれに基づいて作成された。\\

データセットの作成者はサブセクションをトピックと同一視し、この研究でもその仮定を使う。\\

具体的な方法は上記と同じで、すべてのセクションを統合し、モデルにセクションを再分割させる。\\

\section{コメント}

抽出的要約に限らず、トピックを理解したモデルは色んなことに役立てると思う。例えば、対話システムに、ユーザーとの対話が新しいトピックに転換したと判断できれば、古い文脈の破棄ができる、全部の文脈を記憶しなくて済む。インターネットで大量の情報を処理する時にも役立てるかと思う。

\end{document}
