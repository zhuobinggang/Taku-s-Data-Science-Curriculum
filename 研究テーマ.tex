\documentclass[a4paper]{article}
\usepackage{xeCJK}
\setCJKmainfont{IPAMincho}
\setCJKsansfont{IPAGothic}
\setCJKmonofont{IPAGothic}
\usepackage{float}
\restylefloat{table}
\restylefloat{figure}
\usepackage{graphicx}
\title{企画書}
\date{2020年10月20日}
\author{卓 秉綱}

\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}

\addbibresource{qhe.bib}


\begin{document}
\maketitle
研究テーマ：セクション分割による文書のトピック分割

\section{概要}

一つの文章には通常、複数のトピックが含まれている。例えば、とあるフランスの紹介文章に、第一部分のトピックはフランスの地理環境、第二部分はフランスの歴史である。通常、筆者は段落を分けるが、トピックを意図的に分けることはない。\\

しかし、既存のさまざまなテキスト分析システム（例えば、自動要約、情報検索 質問応答）には、この問題が常に無視されている。本研究では、テキスト分析する前にトピックのセグメンテーションを実行すると、より良い成果を出されると想定する(たとえば、トピックごとに自動要約を行い、その結果を組み合わせると、自動要約の精度の向上が期待できる)。そのため、新たなトピック分割の手法を提案する。\\

\section{先行研究}

トピック分割に関する多くの研究があるが 、ニューラルネットワークを使用した研究はまれである。Arnoldらの研究\cite{arnold2019sector}は、まず文をword2vecで特徴ベクトルに転換し、BiLSTMを使って両方向の文脈に基づきその文を複数のトピックカテゴリに分類する。\\

この研究との違いは：1）word2vecの代わりにSentence-bertを使用して文の特徴ベクトルを取得する。 2）トピックの分割にはBiLSTMの代わりにポインターネットワークを使用する。そして、評価するとき両者の結果を比較する。 3）トピックの分類を考慮しない。本研究の目的は、一般的なセクション分割モデルを構築することである。トピック分類には、固定数のカテゴリーラベルが必要で、幅広いアプリケーションシナリオに対応できないため、本研究はトピックを分類しない。


\section{有用性}

本研究で提案する訓練手法を通じて、トピック分割モデルを獲得できる。トピック分割モデルは、自動要約、情報検索質問応答など様々なテキスト分析システムの精度の向上に役立てると想定する。

\section{提案手法}

\begin{figure}[H]
  \includegraphics[width=\columnwidth]{imgs/01.jpg}
  \caption{トピック分割モデルの構造と訓練の流れ}
  \label{fig:img1}
\end{figure}

大体の流れは図\ref{fig:img1}のようである。まず、文全体をSentence-Bert\cite{reimers2019sentence}で特徴ベクトルに転換し、ポインタネットワーク\cite{vinyals2015pointer}に入力してから、分割すべきの複数の文の位置を出力する。その結果に基づき、文書のトピック分割は簡単にできる。\\

トピック分割モデルを予備訓練するために、自己教師あり(self-supervised)の学習手法を提案する：コーパスから文書を読み取り、その文書の全ての段落を一つに合わせ、モデルに段落の再分割をさせる。その訓練により、モデルがトピックへの理解を学習できると期待する。\\

予備訓練した後、WikiSectionを用いてセクション分割をさせてモデルを微調整する。\\

\subsection{Sentence-bert}

図\ref{fig:img1}の左辺のように、複数の文を入力し、Sentence-bertで転換し、同じ数の埋め込みを出力する。ゆえに、Sentence-bertを使って、簡単に文を埋め込みに変換し、各文間のユークリッド距離を比較できる。

\subsubsection{BERTとの比較}
BERTを使って文を埋め込みに変換する場合は、いくつかの裏技を使用しなければならない。先行研究によると、このような方法で得る結果はword2vecの場合とくらべれば非常に良いとは言えない。Sentence bertはこの問題を解決するために生まれ、複数のアプリケーションシナリオで最先端（2019）の成果を取得しました。

\subsubsection{word2vecとの比較}

word2vecで変換後、コンテキストが何であれ、各単語は固定の埋め込みを取得する。それは問題になる。たとえば、単語meanの場合： A mean woman（意地悪な女性）と What do you mean（どういう意味ですか）の単語意味は明らかに異なる。 それに対して、BERTはコンテキストを考慮しているるため、さまざまなアプリケーションシナリオでより良い成果が出る。

\subsection{Pointer-network}

ポインタネットワークを使って、図\ref{fig:img2}のように、入力シーケンスへの複数のポインタを出力できる。 本研究では、文章内のすべての文を入力し、ポインタをセクションの分割点として出力する。

\begin{figure}[H]
  \includegraphics[width=\columnwidth]{imgs/ptnw.png}
  \caption{Attentionメカニズムに基づくPointer-network}
  \label{fig:img2}
\end{figure}

\subsection{WikiSection}

データセットの一部は図\ref{fig:img3}のようである。このデータセットは、ヴィキペディアに基づいて生成された。 一つのヴィキ記事には複数のサブセクションが存在する。たとえば、フランスに関する記事は、歴史や地理などの複数のサブセクションに分割されている。このデータセットは、それらの記事を使いやすい形に整理した。

\begin{figure}
  \includegraphics[width=\columnwidth]{imgs/dataset.png}
  \caption{WikiSection}
  \label{fig:img3}
\end{figure}

\section{実験手法}

\subsection{予備訓練}

まず、インターネットから多数の文章を取得し、文章の全ての段落を一つに合わせ、トピック分割モデルに段落の再分割をさせ、予備訓練する。

\subsection{微調整}

予備訓練した後、WikiSectionを使用してトピック分割モデルを微調整する。微調整の手法は予備訓練の時とほぼ同じである。

\subsection{評価方法}

各文が分割位置であるかどうかを判断し、正解率を取得、Fスコアを計算できる。\\

対照として、3つの実験手法を提案する：1）全ての文は分割位置ではないと判断する方法をベースラインとする。2）BERTによるセクション分割。3）biLSTMによるセクション分割。

\printbibliography

\end{document}
